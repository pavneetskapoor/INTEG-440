{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e3f151ab2156cd69",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<br><br><font color=\"gray\">INTEG 440 / 640<br>MODULE 8 of *Doing Computational Social Science*</font>\n",
    "\n",
    "\n",
    "# <font color=\"green\" size=40>PROCESSING NATURAL <br>LANGUAGE DATA</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "Dr. [John McLevey](http://www.johnmclevey.com)    \n",
    "Department of Knowledge Integration   \n",
    "Department of Sociology & Legal Studies     \n",
    "University of Waterloo         \n",
    "\n",
    "<hr>\n",
    "\n",
    "* INTEG 440 (Undergraduate): This module is worth <font color='#437AB2'>**8%**</font> of your final grade. The questions in this module add up to 10 points. \n",
    "* INTEG 640 (Graduate): This module is worth <font color='#437AB2'>**5%**</font> of your final grade. The questions in this module add up to 10 points. \n",
    "\n",
    "<hr>\n",
    "\n",
    "# Table of Contents \n",
    "\n",
    "* [Overview](#o)\n",
    "* [Learning Outcomes](#lo) \n",
    "* [Prerequisite Knowledge](#pk) \n",
    "* [Assigned Readings](#ar) \n",
    "* [Question Links](#ql)\n",
    "* [Packages Used in this Module](#packs)\n",
    "* [Data Used in this Module](#data)\n",
    "* [**Content Analysis and Computation**](#cac)\n",
    "* [**Fundamentals of Natural Language Processing**](#fundamentals)\n",
    "* [**The `spacy` Pipeline and `Doc` Object**](#spacy)\n",
    "* [References](#refs)\n",
    "\n",
    "<hr>   \n",
    "\n",
    "# Overview <a id='o'></a>\n",
    "\n",
    "In this module, you will learn to use the package `spaCy` for fast and accurate natural language processing and for exploratory text analysis. We will cover fundamental processing tasks such as tokenization, removing stopwords, normalizing text by lemmatization, tagging words by their part-of-speech (e.g. nouns, noun chunks, verbs, adjectives), and extracting information about named entities (e.g. people, places, organizations). \n",
    "\n",
    "<hr>\n",
    "\n",
    "# Learning Outcomes  <a id='lo'></a>\n",
    "\n",
    "Upon successful completion of this module, you will be able to: \n",
    "\n",
    "1. Explain how natural language processing and computational text analysis fits into established traditions of content analysis in the social sciences\n",
    "2. Provide a high-level overview of the fundamental design differences between the most widely-used packages for natural language processing  \n",
    "3. Explain the role that pre-processing plays in text analysis    \n",
    "4. Pre-process text by:  \n",
    "    4.1 removing stopwords   \n",
    "    4.2 selecting parts-of-speech (e.g. nouns) to include in your analysis   \n",
    "    4.3 detect $n$-grams   \n",
    "    4.4 normalize text by lemmatization   \n",
    "    4.5 extract information about named entities (people, places, organizations, etc.)   \n",
    "\n",
    "<hr>\n",
    "\n",
    "# Prerequisite Knowledge  <a id='pk'></a>\n",
    "\n",
    "This module requires a basic level of comfort working with strings, lists, matrices, and `Pandas` dataframes. \n",
    "\n",
    "<hr>\n",
    "\n",
    "# Assigned Readings  <a id='ar'></a>\n",
    "\n",
    "This module assumes you have completed the assigned readings, which are listed immediately below. The readings provide a detailed explanation of the core concepts covered in this module. \n",
    "\n",
    "* <font color=\"green\">Chapter 14 \"Content Analysis and Computation\" from *Doing Computational Social Science*.</font> \n",
    "* <font color=\"green\">Chapter 15 \"Natural Language Processing\" from *Doing Computational Social Science*.</font> \n",
    "\n",
    "As always, I recommend that you (1) complete the assigned readings, (2) attempt to complete this module without consulting the readings, making notes to indicate where you are uncertain, (3) go back to the readings to fill in the gaps in your knowledge, and finally (4) attempt to complete the parts of this module that you were unable to complete the first time around.\n",
    "\n",
    "This module notebook includes highly condensed overviews of *some* of the key material from the assigned reading. This is intended as a *supplement* to the assigned reading, *not as a replacement for it*. These high-level summaries do not contain enough information for you to successfully complete the exercises that are part of this module, and they do not cover every relevant topic. \n",
    "\n",
    "<hr>\n",
    "\n",
    "# Question Links <a id='ql'></a>\n",
    "\n",
    "Make sure you have answered all of the following questions before submitting this notebook on LEARN. \n",
    "\n",
    "1. [Question 1](#yt1) \n",
    "2. [Question 2](#yt2) \n",
    "3. [Question 3](#yt3) \n",
    "4. [Question 4](#yt4) \n",
    "5. [Question 5](#yt5) \n",
    "6. [Question 6](#yt6) \n",
    "7. [Question 7](#yt7) \n",
    "8. [Question 8](#yt8) \n",
    "\n",
    "<hr>\n",
    "\n",
    "# Packages Used in this Module  <a id='packs'></a>\n",
    "\n",
    "The cell below imports the packages that are necessary to complete this module. If there are any additional packages you wish to import, you may add them to this import cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-31df2a53488238a0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 64 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting smart-open>=1.8.1\n",
      "  Downloading smart_open-1.9.0.tar.gz (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 2.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: boto>=2.32 in /opt/conda/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.12.16-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 45.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.7)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 1.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.9.5-py2.py3-none-any.whl (24 kB)\n",
      "Collecting botocore<1.16.0,>=1.15.16\n",
      "  Downloading botocore-1.15.16-py2.py3-none-any.whl (5.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9 MB 40.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.16->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "\u001b[K     |████████████████████████████████| 547 kB 61.0 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-1.9.0-py3-none-any.whl size=73085 sha256=7ac112c20681b576881f42103482fab7334b973ec8fd8706f9f764a6599f6dba\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/1f/f6/d9/49cfa288fb14de91f127fb6f7a5a29f389c9588a41140107d9\n",
      "Successfully built smart-open\n",
      "Installing collected packages: jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.12.16 botocore-1.15.16 docutils-0.15.2 gensim-3.8.1 jmespath-0.9.5 s3transfer-0.3.3 smart-open-1.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "import pandas as pd\n",
    "import spacy \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ba8f68f6d3d71f2b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Data Used in this Module <a id='data'></a>\n",
    "\n",
    "Most of this module (and the three that follow) will use Megan Risdale's (2016) dataset on fake and real news ([obtained from Kaggle](https://www.kaggle.com/mrisdal/fake-news)). We will load it from our data subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f5c196e5d407d30e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5554</th>\n",
       "      <td>7618</td>\n",
       "      <td>JUST IN: Republicans Sued Over Trump’s Call To...</td>\n",
       "      <td>\\nThe Democratic National Committee just hau...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5487</th>\n",
       "      <td>3531</td>\n",
       "      <td>Iowa Christians struggle to square faith with ...</td>\n",
       "      <td>Des Moines, Iowa (CNN) As Christians, they wan...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5540</th>\n",
       "      <td>6759</td>\n",
       "      <td>Fukushima – The Untouchable Eco-Apocalypse No ...</td>\n",
       "      <td>Waking Times – by Alex Pietrowski \\nThe most i...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4752</th>\n",
       "      <td>35</td>\n",
       "      <td>Arizona first in nation to require patients be...</td>\n",
       "      <td>Arizona will become the first state in the nat...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4475</th>\n",
       "      <td>3333</td>\n",
       "      <td>Clinton Foundation received subpoena from Stat...</td>\n",
       "      <td>Investigators with the State Department issued...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>3592</td>\n",
       "      <td>In France, a growing debate over why some spee...</td>\n",
       "      <td>They tease terrorists. The prophet Muhammad cr...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3039</th>\n",
       "      <td>8924</td>\n",
       "      <td>Without Bold Agenda, Warn Progressives, A Clin...</td>\n",
       "      <td>Without Bold Agenda, Warn Progressives, A Clin...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3257</th>\n",
       "      <td>4700</td>\n",
       "      <td>What WikiLeaks hack says about Clinton: Our view</td>\n",
       "      <td>Now we know why she didn't want those Wall Str...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3255</td>\n",
       "      <td>Give Social Security recipients a CEO-style raise</td>\n",
       "      <td>(CNN) On Veterans Day we recognize and honor t...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>2092</td>\n",
       "      <td>On climate change, ideological and partisan po...</td>\n",
       "      <td>Later this week, Pope Francis will reportedly ...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>3463</td>\n",
       "      <td>The Supreme Court Case That Could Gut Obamacar...</td>\n",
       "      <td>UPDATE: June 4 -- The health insurance enrollm...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2886</th>\n",
       "      <td>5684</td>\n",
       "      <td>Comment on Tutorial: Riding The Philippine Jee...</td>\n",
       "      <td>adobochron 1 Comment \\nMANILA, Philippines (Th...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2863</th>\n",
       "      <td>8010</td>\n",
       "      <td>Hillary FRANTIC As Dirty Secret Implodes, Gets...</td>\n",
       "      <td>Share This \\nHillary Clinton thought her email...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>4582</td>\n",
       "      <td>Protesting Donald Trump’s Election, Not Wars, ...</td>\n",
       "      <td>Protests and vigils have erupted in major citi...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>5032</td>\n",
       "      <td>Why Koch and Republican donor network won’t ba...</td>\n",
       "      <td>Charles Koch and his network of conservative d...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              title  \\\n",
       "5554        7618  JUST IN: Republicans Sued Over Trump’s Call To...   \n",
       "5487        3531  Iowa Christians struggle to square faith with ...   \n",
       "5540        6759  Fukushima – The Untouchable Eco-Apocalypse No ...   \n",
       "4752          35  Arizona first in nation to require patients be...   \n",
       "4475        3333  Clinton Foundation received subpoena from Stat...   \n",
       "281         3592  In France, a growing debate over why some spee...   \n",
       "3039        8924  Without Bold Agenda, Warn Progressives, A Clin...   \n",
       "3257        4700   What WikiLeaks hack says about Clinton: Our view   \n",
       "97          3255  Give Social Security recipients a CEO-style raise   \n",
       "1189        2092  On climate change, ideological and partisan po...   \n",
       "1456        3463  The Supreme Court Case That Could Gut Obamacar...   \n",
       "2886        5684  Comment on Tutorial: Riding The Philippine Jee...   \n",
       "2863        8010  Hillary FRANTIC As Dirty Secret Implodes, Gets...   \n",
       "1616        4582  Protesting Donald Trump’s Election, Not Wars, ...   \n",
       "1346        5032  Why Koch and Republican donor network won’t ba...   \n",
       "\n",
       "                                                   text label  \n",
       "5554    \\nThe Democratic National Committee just hau...  FAKE  \n",
       "5487  Des Moines, Iowa (CNN) As Christians, they wan...  REAL  \n",
       "5540  Waking Times – by Alex Pietrowski \\nThe most i...  FAKE  \n",
       "4752  Arizona will become the first state in the nat...  REAL  \n",
       "4475  Investigators with the State Department issued...  REAL  \n",
       "281   They tease terrorists. The prophet Muhammad cr...  REAL  \n",
       "3039  Without Bold Agenda, Warn Progressives, A Clin...  FAKE  \n",
       "3257  Now we know why she didn't want those Wall Str...  REAL  \n",
       "97    (CNN) On Veterans Day we recognize and honor t...  REAL  \n",
       "1189  Later this week, Pope Francis will reportedly ...  REAL  \n",
       "1456  UPDATE: June 4 -- The health insurance enrollm...  REAL  \n",
       "2886  adobochron 1 Comment \\nMANILA, Philippines (Th...  FAKE  \n",
       "2863  Share This \\nHillary Clinton thought her email...  FAKE  \n",
       "1616  Protests and vigils have erupted in major citi...  REAL  \n",
       "1346  Charles Koch and his network of conservative d...  REAL  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/fake_news.csv')\n",
    "df.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9de58fe2de275a25",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Content Analysis and Computation <a id='cac'></a>\n",
    "\n",
    "Chapter 14 ('Content Analysis and Computation') from *Doing Computational Social Science* provides a high-level overview of the goals of content analysis in the social sciences, and the challenges and opportunities associated with digital text data and new computational methods. Before getting into specific methods, it is important that you understand the differences between two fundamentally different types of computational text analysis (supervised and unsupervised), and how they can be combined with careful human reading and interpretation. \n",
    "\n",
    "### <font color=\"green\">YOUR TURN! (Question 1)</font> <a id='yt1'></a>\n",
    "\n",
    "Question is Worth: <font color=\"green\">1.25 points</font>\n",
    "\n",
    "In the cell block below, compare the uses of and workflows for supervised and unsupervised learning. Explain how both relate to longstanding traditions of content analysis in the social sciences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f65de8f227b052b8",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Supervised learning helps to scale operationalized content and models of content analysis. They help for researchers to evaluate theory and have much control of the content machines will effectively \"learn\". For points of analysis for which you might already know of the end result, supervised learning helps to effectively develop a system that can be trained on criteria to evaluate data and classify effectively in the end. Within this methodology, there is more assurance that the data can be referenced to a ground truth for a deeper and sound approach of analysis. Essentially, we can control many things within the data here. \n",
    "\n",
    "Unsupervised learning uncovers meaning within data that has previously not been assessed for patterns. Without pertaining labels within the data, this approach implements inductive methods to develop new patterns or find meaning hidden within the data. In its inherent manner, discover of new theory and pattern is relevant to the world of social sciences as long as it can be interpreted correctly. In comparison to supervised learning, this will require some digging and appropriate ground of theoretical application which will allow for some interesting new patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-21225bb399e8bd92",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### <font color=\"green\">YOUR TURN! (Question 2)</font> <a id='yt2'></a>\n",
    "\n",
    "Question is Worth: <font color=\"green\">1.25 points</font>\n",
    "\n",
    "Methodologists have recently been writing about combining unsupervised learning, supervised learning, and 'guided' human reading and interpretation. In the cell block below, explain (1) why it is essential to preserve the role of human interpretation in computational text analysis, and (2) why it can be useful to combine unsupervised and supervised learning in formal frameworks such as Laura Nelson's (2017) '[Computational Grounded Theory](https://journals.sagepub.com/doi/abs/10.1177/0049124117729703)' (which was discussed in the assigned reading). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-eb3c296c90415c53",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Why is it essential to preserve the role of human interpretation in computational text analysis?\n",
    "\n",
    "Machines and humans are inherently different and can operate in harmony if used in the right manner. The context and level of interpretation of the intracacies of a language that humans possess is unmatched by any algorithm that could be used by machine learning to uncover meaning. Computers do mathematical operations on data rather than understand the meaning and the linguistics associated. Computers are used for scaled operations while humans are good for being . the ground truth of reference. \n",
    "\n",
    "Why can it be useful to combine unsupervised and supervised learning in formal frameworks?\n",
    "\n",
    "Unsupervised learning methods can help to discover patterns and concepts that can act as ground truth for the data once validated by researchers. In this form, the process of labelling and ground truth development to structure data is effectively removed. With a mixture of guided deep reading, essentially supervised methodologies, to avoid outliers leading to biases and making the process of combination easier with interpretation of the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9537e87dc3bef50e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Fundamentals of Natural Language Processing <a id='fundamentals'></a>\n",
    "\n",
    "## Getting to Know `spacy` \n",
    "\n",
    "### <font color=\"green\">YOUR TURN! (Question 3)</font> <a id='yt3'></a>\n",
    "\n",
    "Question is Worth: <font color=\"green\">.5 points</font>\n",
    "\n",
    "As discussed in the assigned reading, it is important to understand the design philosophy behind `spaCy` and how it differs from alternatives, including the more established package `nltk`. Before we start getting into how `spaCy` works, take a few moments to summarize the design philosophy for `spaCy` in the text cell below. Be sure to compare this design philosophy with the one guiding `nltk`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9cdcce0ac5be9723",
     "locked": false,
     "points": 0.25,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    " In relative comparison to `nltk`, `spaCy` owner has mentioned that the package contains inefficient algorithms that were orginally designed for teaching NLP. Inherently, they provide a great reference for beginners looking to learn the world of NLP but do not provide the speed and performance necessary for those that need to do practical computations. `spaCy` provides many constraints as a package in order to account for higher performance processing in context for production level NLP rather than research-oriented. This helps in making informed decision making for some scientists but leaves a lot to answer for those that need to go outside the boundaries and explore. Versions of `spaCy`can also affect the type of analysis done since packages are always constantly updating. As long as you're able to consider how the data research will be done and are mindful of the constraints in the design of the package, you can tweak the algorithm and train for the data how you would like for it to be trained. Alongside, `spaCy` believes in the quality of data being paramount to the performance of NLP for which it provides to tools to develop meticulous and caring measures of data annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1dffb0ad6d3c6bd1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# The `spacy` Pipeline and `Doc` Object <a id='spacy'></a>\n",
    "\n",
    "The following abstract is from Muller, Sampson, and Winter's (2018) article \"[Environmental Inequality: The Social Causes and Consequences of Lead Exposure](https://www.annualreviews.org/doi/10.1146/annurev-soc-073117-041222),\" published in the *Annual Review of Sociology*. Let's use this example to illustrate some basic natural language processing (NLP) concepts and tasks before moving on to a bigger example.\n",
    "\n",
    "> In this article, we review evidence from the social and medical sciences on the causes and effects of lead exposure. We argue that lead exposure is an important subject for sociological analysis because it is socially stratified and has important social consequences -- consequences that themselves depend in part on children's social environments. We present a model of environmental inequality over the life course to guide an agenda for future research. We conclude with a call for deeper exchange between urban sociology, environmental sociology, and public health, and for more collaboration between scholars and local communities in the pursuit of independent science for the common good.\n",
    "\n",
    "### <font color=\"green\">YOUR TURN! (Question 4)</font> <a id='yt4'></a>\n",
    "\n",
    "Question is Worth: <font color=\"green\">.5 points</font>\n",
    "\n",
    "When you read this abstract, you know where words begin and end, and where sentences begin and end. It is more challenging for a computer to be able to tell where these \"tokens\" begin and end. Why? What are some common challenges for a computer \"reading\" text data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c26aa816968f2118",
     "locked": false,
     "points": 0.25,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "It's hard to assess where tokens begin or end because of different context and meaning of different rules in grammar can make it tricky for computers to assess where the beginning and end lies. Things like punctuation vs acronym, contraction or expansion in token development, or emoji can be hard to interpret. To combat this context and language-specific tokenization rules help to make intepretation easier overall alongside using model-approaches for analysis. Addditionally, understanding parts of the speech are important in order for machines to accurately learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-806df6ee44d205cb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Pre-processing tasks like selecting words based on their part-of-speech always degrades the reading experience for a human because it involves stripping out information that makes it easier for humans to understand the meaning of any individual text. But when we want a computer to tell us something about the content of many texts in a document collection, the same pre-processing tasks are *essential* for producing informative results. Effective pre-processing is all about knowing what kinds of information needs to be preserved or removed to improve the ability of the computer to \"read\" many texts. That may or may not involve tasks like selecting nouns and verbs, but it will almost always include tasks like removing stopwords, punctuation, and normalizing text. \n",
    "\n",
    "Let's begin by walking through some basic pre-processing on the abstract introduced above. The first thing we will do is create a string object containing the abstract, and then we will feed it into the `spaCy` pipeline `nlp()`, which we defined right under `import spacy` when we were importing the packages used in this lesson. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ff7f4b687b756cf2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "ab = \"In this article, we review evidence from the social and medical sciences on the causes and effects of lead exposure. We argue that lead exposure is an important subject for sociological analysis because it is socially stratified and has important social consequences -- consequences that themselves depend in part on children's social environments. We present a model of environmental inequality over the life course to guide an agenda for future research. We conclude with a call for deeper exchange between urban sociology, environmental sociology, and public health, and for more collaboration between scholars and local communities in the pursuit of independent science for the common good.\"\n",
    "\n",
    "proc = nlp(ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7ce2298421a5bfca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "When we process text by running it through the `spaCy` pipeline, `spaCy` stores the information we want in a `Doc` object. If we want to see the tokenized sentences, for example, we can iterate over the sentences in the `Doc` object and print them to screen. In this example, our `Doc` object is stored in the variable `proc.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e68ca4578f2510ce",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this article, we review evidence from the social and medical sciences on the causes and effects of lead exposure.\n",
      "\n",
      "\n",
      "We argue that lead exposure is an important subject for sociological analysis because it is socially stratified and has important social consequences -- consequences that themselves depend in part on children's social environments.\n",
      "\n",
      "\n",
      "We present a model of environmental inequality over the life course to guide an agenda for future research.\n",
      "\n",
      "\n",
      "We conclude with a call for deeper exchange between urban sociology, environmental sociology, and public health, and for more collaboration between scholars and local communities in the pursuit of independent science for the common good.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in proc.sents:\n",
    "    print(sent)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d0b5c1cd041a77f0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We can iterate over tokens (e.g. sentences, words) for a variety of important text processing tasks, including normalizing text, removing stopwords, identifying parts-of-speech, and extracting named entities. For example, we can use normalized words rather than the original words by iterating over the words in the abstract and adding each word's lemma to a list. This time we will use `list comprehension` to iterate over the tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e625b04d923e22f4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'this', 'article', ',', '-PRON-', 'review', 'evidence', 'from', 'the', 'social', 'and', 'medical', 'science', 'on', 'the', 'cause', 'and', 'effect', 'of', 'lead', 'exposure', '.', '-PRON-', 'argue', 'that', 'lead', 'exposure', 'be', 'an', 'important', 'subject', 'for', 'sociological', 'analysis', 'because', '-PRON-', 'be', 'socially', 'stratified', 'and', 'have', 'important', 'social', 'consequence', '--', 'consequence', 'that', '-PRON-', 'depend', 'in', 'part', 'on', 'child', \"'s\", 'social', 'environment', '.', '-PRON-', 'present', 'a', 'model', 'of', 'environmental', 'inequality', 'over', 'the', 'life', 'course', 'to', 'guide', 'an', 'agenda', 'for', 'future', 'research', '.', '-PRON-', 'conclude', 'with', 'a', 'call', 'for', 'deep', 'exchange', 'between', 'urban', 'sociology', ',', 'environmental', 'sociology', ',', 'and', 'public', 'health', ',', 'and', 'for', 'more', 'collaboration', 'between', 'scholar', 'and', 'local', 'community', 'in', 'the', 'pursuit', 'of', 'independent', 'science', 'for', 'the', 'common', 'good', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmas = [token.lemma_ for token in proc]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1aa6889f30aef467",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Similarly, we can iterate over the words in the abstract and check to see if the word is a stopword. If not, we can add it to a new list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bbdc7acc8bdc5545",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[article, ,, review, evidence, social, medical, sciences, causes, effects, lead, exposure, ., argue, lead, exposure, important, subject, sociological, analysis, socially, stratified, important, social, consequences, --, consequences, depend, children, social, environments, ., present, model, environmental, inequality, life, course, guide, agenda, future, research, ., conclude, deeper, exchange, urban, sociology, ,, environmental, sociology, ,, public, health, ,, collaboration, scholars, local, communities, pursuit, independent, science, common, good, .]\n"
     ]
    }
   ],
   "source": [
    "wo_stops = [token for token in proc if token.is_stop == False]\n",
    "print(wo_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-73da9ded116aab12",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Extracting words by their part-of-speech is no different. First, let's print the part-of-speech for each word in the abstract. Then let's make a list that includes only the nouns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8319b0bda2c5bcca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In(ADP)\n",
      "this(DET)\n",
      "article(NOUN)\n",
      ",(PUNCT)\n",
      "we(PRON)\n",
      "review(VERB)\n",
      "evidence(NOUN)\n",
      "from(ADP)\n",
      "the(DET)\n",
      "social(ADJ)\n",
      "and(CCONJ)\n",
      "medical(ADJ)\n",
      "sciences(NOUN)\n",
      "on(ADP)\n",
      "the(DET)\n",
      "causes(NOUN)\n",
      "and(CCONJ)\n",
      "effects(NOUN)\n",
      "of(ADP)\n",
      "lead(NOUN)\n",
      "exposure(NOUN)\n",
      ".(PUNCT)\n",
      "We(PRON)\n",
      "argue(VERB)\n",
      "that(DET)\n",
      "lead(NOUN)\n",
      "exposure(NOUN)\n",
      "is(AUX)\n",
      "an(DET)\n",
      "important(ADJ)\n",
      "subject(NOUN)\n",
      "for(ADP)\n",
      "sociological(ADJ)\n",
      "analysis(NOUN)\n",
      "because(SCONJ)\n",
      "it(PRON)\n",
      "is(AUX)\n",
      "socially(ADV)\n",
      "stratified(ADJ)\n",
      "and(CCONJ)\n",
      "has(AUX)\n",
      "important(ADJ)\n",
      "social(ADJ)\n",
      "consequences(NOUN)\n",
      "--(PUNCT)\n",
      "consequences(NOUN)\n",
      "that(SCONJ)\n",
      "themselves(PRON)\n",
      "depend(VERB)\n",
      "in(ADP)\n",
      "part(NOUN)\n",
      "on(ADP)\n",
      "children(NOUN)\n",
      "'s(PART)\n",
      "social(ADJ)\n",
      "environments(NOUN)\n",
      ".(PUNCT)\n",
      "We(PRON)\n",
      "present(VERB)\n",
      "a(DET)\n",
      "model(NOUN)\n",
      "of(ADP)\n",
      "environmental(ADJ)\n",
      "inequality(NOUN)\n",
      "over(ADP)\n",
      "the(DET)\n",
      "life(NOUN)\n",
      "course(NOUN)\n",
      "to(PART)\n",
      "guide(VERB)\n",
      "an(DET)\n",
      "agenda(NOUN)\n",
      "for(ADP)\n",
      "future(ADJ)\n",
      "research(NOUN)\n",
      ".(PUNCT)\n",
      "We(PRON)\n",
      "conclude(VERB)\n",
      "with(ADP)\n",
      "a(DET)\n",
      "call(NOUN)\n",
      "for(ADP)\n",
      "deeper(ADJ)\n",
      "exchange(NOUN)\n",
      "between(ADP)\n",
      "urban(ADJ)\n",
      "sociology(NOUN)\n",
      ",(PUNCT)\n",
      "environmental(ADJ)\n",
      "sociology(NOUN)\n",
      ",(PUNCT)\n",
      "and(CCONJ)\n",
      "public(ADJ)\n",
      "health(NOUN)\n",
      ",(PUNCT)\n",
      "and(CCONJ)\n",
      "for(ADP)\n",
      "more(ADJ)\n",
      "collaboration(NOUN)\n",
      "between(ADP)\n",
      "scholars(NOUN)\n",
      "and(CCONJ)\n",
      "local(ADJ)\n",
      "communities(NOUN)\n",
      "in(ADP)\n",
      "the(DET)\n",
      "pursuit(NOUN)\n",
      "of(ADP)\n",
      "independent(ADJ)\n",
      "science(NOUN)\n",
      "for(ADP)\n",
      "the(DET)\n",
      "common(ADJ)\n",
      "good(NOUN)\n",
      ".(PUNCT)\n"
     ]
    }
   ],
   "source": [
    "for item in proc:\n",
    "    print(item.text + '({})'.format(item.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e93d9a1fd2277584",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### <font color=\"green\">YOUR TURN! (Question 5)</font> <a id='yt5'></a>\n",
    "\n",
    "Question is Worth: <font color=\"green\">1 point</font>\n",
    "\n",
    "In the cell below, use either a for loop or list comprehension to iterate over the tokens and add the word to a list of nouns if the word is a noun. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-43c7c8a1780b43d1",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['article', 'evidence', 'sciences', 'causes', 'effects', 'lead', 'exposure', 'lead', 'exposure', 'subject', 'analysis', 'consequences', 'consequences', 'part', 'children', 'environments', 'model', 'inequality', 'life', 'course', 'agenda', 'research', 'call', 'exchange', 'sociology', 'sociology', 'health', 'collaboration', 'scholars', 'communities', 'pursuit', 'science', 'good']\n"
     ]
    }
   ],
   "source": [
    "## Your Answer Here ## \n",
    "\n",
    "### BEGIN SOLUTION \n",
    "nouns = [item.text for item in proc if item.pos_ == 'NOUN']\n",
    "### END SOLUTION \n",
    "\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b6c596de005d0fd7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "`spaCy` is also able to identify noun chunks, or \"phrases.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1039b8a8b7d9af7b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for item in proc.noun_chunks:\n",
    "  print(item.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6cfab4c0e49e4b00",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "As you can see, `spaCy` really simplifies key tasks in natural language processing. Knowing how, when, and why to do these and other tasks is the secret to getting good results when you are doing automated text analysis. **If you don't pre-process your text, you will not get good results, no matter how sophisticated your models are.** \n",
    "\n",
    "How, then, can we combine these methods into a simple text pre-processing step? And how do we scale this up to a collection of abstracts (or any other text data) rather than a single string?\n",
    "\n",
    "In the cells below, I have provided some code to pre-process text data from a sample of our fake news dataset. We imported that data into memory at the start of this notebook. \n",
    "\n",
    "The cell immediately below this one will take a bit of time to run because of all the work `spaCy` is doing to parse the text. Once it has finished, run the next code cell to actually pre-process the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5b727d02214f64fe",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_sample = df[df['label'] == 'FAKE'].sample(200)\n",
    "real_sample = df[df['label'] == 'REAL'].sample(200)\n",
    "sampled_news = pd.concat([fake_sample, real_sample])\n",
    "len(sampled_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0ae4a413d76b6b21",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "text = sampled_news['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the next step will take a while to run because `spaCy` is doing all of the computationally intensive work up front. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4c1e0ecf3da2a464",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "processed = [nlp(t) for t in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3d9364982b874003",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now let's write a bit of code to iterate over the list of texts that have been parsed by `spaCy`. Recall from the lesson on computational thinking that we generally want to develop solutions that can cover a range of related problems. In this case, we want to write code that can be used to process any text we provide it, not *just* the text we gave it in any single instance. Below, we will write a function to do this work. \n",
    "\n",
    "Our goal is to return a list of lists. Each abstract in our dataset will be represented by a list of nouns and adjectives. That list will then be appended to a list of abstracts in the dataset. *In addition to our list of nouns and adjectives*, we will extract a list of \"named entities\" of the type `person` using `spacy`'s named entity recognition models. The named entities will also be returned as a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7758e8848ba625f3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def prepare_text(list_of_processed_texts):\n",
    "    \"\"\"\n",
    "    Quickly grab entities and lemmas of non-stopword nouns and adjectives. \n",
    "    \"\"\"\n",
    "    analysis_text = []\n",
    "    named_entities = []\n",
    "    \n",
    "    types = ['NOUN', 'ADJ'] \n",
    "    for doc in list_of_processed_texts: \n",
    "        ents = [ent.text for ent in doc.ents if ent.label_ is 'PERSON' and ' ' in ent.text]    \n",
    "        reduced = [token.lemma_ for token in doc if token.is_stop is False and token.pos_ in types]\n",
    "        \n",
    "        analysis_text.append(\" \".join(reduced))\n",
    "        named_entities.append(list(set(ents)))\n",
    "    return analysis_text, named_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-609d4ef346b4fcf2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### <font color=\"green\">YOUR TURN! (Question 6)</font>  <a id='yt6'></a>\n",
    "\n",
    "Question is Worth: <font color=\"green\">2 points</font>\n",
    "    \n",
    "In the text cell below, explain *in plain language* what each line of the function in the code block above is doing. Be sure to explain what each line takes in (i.e. the inputs), what it does to those inputs, and what it returns (i.e. the outputs). What exactly does the full function return? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-bf382dbe80616c1f",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "For lists of processed texts that have gone through the `nlp()` function already, essentially a list of doc objects. Within the function, first lists are initialized for analysis of text and pertaining entities of analysis. A types list is defined to consider the types for the particular analysis and their specific relevance. In this case, only nouns and adjectives will be considered. For each doc object within the sent in function paramter, named entity text (on the condition that entity label is PERSON via a recognition model and text is empty with `' '`) is saved to a variable along with a list of base form tokens (in unicode) are taken from the doc for those that are considered either nouns or adjectives in a non stop-word form. Those respective lists are then appended to `analysis_text` and `named_entities` for each doc and sent back as a list of lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use our function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepped = prepare_text(processed)\n",
    "\n",
    "analysis_text = prepped[0]\n",
    "named_entities = prepped[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the result by looking at the content returned for the first 5 stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['home shocking look shocking late number anchor baby birth baby birthright citizen city size analysis datum datum newborn illegal percent birth analysis report birth unmarried foreign woman foreign illegal birthrate drop decline birth american woman report baby unauthorized immigrant parent percent birth percent birth foreign mother share new mother teenager high % foreign % pic.twitter.com/ydjpv2ngxh @pewresearch birth foreign mother unmarried woman peak percent time rate steady woman percent study birthrate woman rise birthrate immigrant mother growth annual birth immigrant mom pic.twitter.com/doznvxxvgt @pewresearch annual number baby recent year great recession significant drop birth nationwide trajectory past decade upward birth growth number baby immigrant woman immigrant woman birth threefold increase immigrant woman birth annual number birth woman percent time period people case culture look easy answer refugee flow amnesty illegal immigrant law contrast law order wall immigration law safety interest american citizen which critical point country history',\n",
       " 'election high office land inevitable night campaign dark dark medium # itsover # election @thedailysheeple victorious supporter hour official count team proud whatev tonight confirmation victory world mind report concession forefront unprecedented presidential candidate stage thousand distraught party goer coronation firework controversial campaign manager stage election vote moment wrong wrong health episode # # hillary vote # hillaryhealth @thedailysheeple health episode loss close friend confidante fact breakdown condition live television shock % lock bare everybody fault opinion morning old friend friend female friend way hard friend loss president handcuff special prosecutor charge secret deal hook',\n",
       " 'post site criminal investigation alleged child sex infraction couple characteristic awkward wife close companion underage sex investigation electronic communication investigative target grand jury day subpoena device possession family case grand jury supervised email investigation device investigation pertinent email certainty report pertinent email laptop initial investigation grand jury subpoena power possible existence pertinent email husband computing device laptop background information technology consulting affluent powerful people email e mail people im text messaging -PRON- old model software motivation e mail server plenty case powerful person e mail thing good solution people ready aye ready computer alarm thing e mail account point account computer account plenty e mail mention account thing purpose course printer difficult e mail account strange believable stuff theory platform agnostic unsophisticated problem e mail account printer device scenario portable machine reason home printer desktop machine old printer family computer e mail account computer account setup computer computer investigation bam thousand e mail message evidence',\n",
       " 'indra shocking twist grown iraqi man old pool austrian capital trip integration youngster shower toilet cubicle attack rapist pool diving board police old alarm lifeguard child severe anal injury local child hospital post traumatic stress disorder police interview crime officer incident sexual emergency wife sex month court guilty sexual assault rape minor year jail',\n",
       " 'stat amazing election close detail video p lease donate our absolute good food storage satisfied customer late information']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the results are indeed a list of lists, and the content of each is as expected.\n",
    "\n",
    "Now, let's inspect some of the named entities `spaCy` identified. Remember that named entity recognition is much less accurate than part-of-speech tagging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Donald Trump', 'Hillary Clinton'],\n",
       " ['Ed Klein',\n",
       "  'John Podesta',\n",
       "  'Completely Dark –',\n",
       "  'James Comey',\n",
       "  'Clinton Campaign',\n",
       "  'Hillary Clinton',\n",
       "  'Donald Trump'],\n",
       " ['Adam Yoshida',\n",
       "  'Ed Timperlake',\n",
       "  'Lucianne Goldberg',\n",
       "  'Huma Abedin',\n",
       "  'Hillary Clinton',\n",
       "  'Carlos Danger',\n",
       "  'Anthony Weiner'],\n",
       " ['Amir A.', 'Amir A', 'INDRA WARNES \\n'],\n",
       " ['Click Here', 'Dave Hodges'],\n",
       " ['Wendy Kaufman'],\n",
       " ['Damon Smith',\n",
       "  'Simon Eastwood',\n",
       "  'pic.twitter.com/b0BekmI1aw \\n— Mark White',\n",
       "  'Newton Abbot'],\n",
       " [],\n",
       " ['Charlie Hebdo', 'Joe Walsh', 'Simon Maloy'],\n",
       " ['Mark Sykes', 'Arthur James Balfour', 'Walter Rothschild', 'Ramzy Baroud'],\n",
       " ['Anthony A Fabrikant',\n",
       "  'George Bush',\n",
       "  'Edward Snowden',\n",
       "  'Barrack Obama',\n",
       "  'Eddie L.'],\n",
       " ['Geert Wilders',\n",
       "  'Ian Greenhalgh',\n",
       "  'Naming Trump',\n",
       "  'Al Hussein',\n",
       "  'Nigel Farage'],\n",
       " ['Guest Click'],\n",
       " ['Amanda Froelich'],\n",
       " ['Vitamin E']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "named_entities[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7220ff9f957caab3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### <font color=\"green\">YOUR TURN! (Question 7)</font> <a id='yt7'></a>\n",
    "\n",
    "Question is Worth: <font color=\"green\">2 points</font> (1 for the code + 1 for the comparison)\n",
    "\n",
    "In the code block below, produce a list of the 25 most frequently mentioned people for a subset of the fake news stories and the real news stories. Then compare them in the text cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-137d3f7ec4245a2a",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(109, 'Hillary Clinton'),\n",
       " (109, 'Donald Trump'),\n",
       " (41, 'Barack Obama'),\n",
       " (34, 'Ted Cruz'),\n",
       " (27, 'Bill Clinton'),\n",
       " (24, 'Bernie Sanders'),\n",
       " (20, 'Mitch McConnell'),\n",
       " (19, 'George W. Bush'),\n",
       " (18, 'Marco Rubio'),\n",
       " (18, 'John Kasich'),\n",
       " (18, 'Jeb Bush'),\n",
       " (15, 'Paul Ryan'),\n",
       " (14, 'Vladimir Putin'),\n",
       " (12, 'Rand Paul'),\n",
       " (11, 'Ronald Reagan'),\n",
       " (11, 'John Boehner'),\n",
       " (11, 'James Comey'),\n",
       " (10, 'Mike Pence'),\n",
       " (10, \"Donald Trump's\"),\n",
       " (10, 'Ben Carson'),\n",
       " (9, 'Chris Christie'),\n",
       " (8, 'Loretta Lynch'),\n",
       " (8, 'Lindsey Graham'),\n",
       " (8, 'Huma Abedin'),\n",
       " (8, 'Harry Reid')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your Answer Here \n",
    "freq_names = {}\n",
    "for names_list in named_entities:\n",
    "    for name in names_list:\n",
    "        if name in freq_names:\n",
    "            freq_names[name] = freq_names[name]+1\n",
    "        else:\n",
    "            freq_names[name] = 1\n",
    "\n",
    "sorted_d = sorted((value, key) for (key,value) in freq_names.items())\n",
    "sorted_d[::-1][:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f4f46cc0f11dd53c",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "The most frequent ones are \"surprisingly\" the ones involved in American politics (or closely tied to) including notably Hillary Clinton and Donald Trump as those topping the list. One instance has been caught as a contraction with \"Donald Trump's\". Within fake aand real news, these are the figures that are trending and most frequent in real and fake news alike. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">YOUR TURN! (Question 8)</font> <a id='yt8'></a>\n",
    "\n",
    "Question is Worth: <font color=\"green\">1.5 points</font>\n",
    "\n",
    "In the code block below, write a loop or list comprehension to create a list of named entities that are 'Geopolitical Entities' (in `spaCy`, `GPE`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-78e93e599308067d",
     "locked": false,
     "points": 1.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"New York City's\", 'Trumpland', 'Yemen', 'Saudi Arabia', 'U.S.', 'Iraq', 'Syria', 'Honduras', 'Afghanistan', 'Israel', 'Iran', 'Russia', 'Ukraine', 'Algeria', 'Kuwait', 'United Arab Emirates', 'Oman', 'Qatar', 'US', 'Drudge', 'California', 'serfdom.', 'Gettysburg', 'Washington', 'China', 'America', 'Mexico', 'the United States', 'Bakersfield', 'Sacramento', 'D-Sacramento', 'New York', 'Virginia', 'Los Angeles', 'Little Rock', 'New York FBI', 'Brooklyn', 'Manhattan', 'Arkansas', 'Las Vegas', 'North Korea', 'Vise', 'Japan', 'South Korea', \"North Korea's\", 'Tokyo', 'NORTH KOREA', 'Seoul', 'Iowa', 'Aston', 'Fulham', 'France', 'Breitbart', 'Marais Stadium', 'Jerusalem', 'the Dome of the Rock Mosque', 'Old City', 'Palestine', 'UNESCO', 'Bookmark', 'Canaan', 'North Africa', 'Rome', 'CE', 'the Roman Empire', 'tapaDILDO', 'Egypt', 'tel Megiddo', 'Torah', 'OTHERS', 'East Jerusalem', 'Megiddo', 'TapaDILDO', 'a Roman Empire', 'Empire', 'Germany', 'WASHINGTON', 'ISRAEL', 'Lebanon', 'Libya', 'Somalia', 'Turkey', 'Jordan', 'AMERICA', 'Gaza', 'Gaza Strip', 'the State of Israel', 'הן בהגדרת המשימה', 'הן בהגדרת', 'וכמובן', 'גרורותיו', 'הרקטות', 'לאור', 'אספקת החשמל', 'מקסימלי', 'יחוסלו', 'היסוד', 'הרצועה', 'רק', 'NY', 'Barberton', 'New York City', 'Obama', '\\x97', 'Hillary\\x92s', 'N.Korea', 'New Delhi', 'India', 'Old Delhi', 'Draitser', 'the White Helmets', 'UK', 'West Aleppo', 'sarin', 'Yugoslavia', 'Corrente', 'Dallas', 'Texas', 'Mass.', 'Colo.', 'Wash.', 'Pennsylvania', 'New Hampshire', 'Trump', 'Georgia', 'Missouri', 'Nevada', 'Wisconsin', 'North Carolina', 'Florida', 'Michigan', 'Maine', 'Korea', 'Gadani', 'Pakistan', 'Malaysia', 'Singapore', 'Standing Rock', 'Britain', 'New Zealand', 'London', 'Chappaqua', 'Kansas', 'KKKlinton', 'McCarthy', 'Nuremberg', 'Zika', 'Brazil', 'Venezuela', 'Puerto Rico', 'Marinho', 'Mississippi', 'Louisiana', 'Cuba', 'Haiti', 'Curacao', 'hemp', 'NEW DELHI', 'Sputnik', 'Indonesia', 'Kenya', 'Orozco', 'Khartoum', 'Sudan', 'Aleppo', 'Finland', 'Tehran We', 'Tehran', 'CALIFORNIA', 'DELAWARE', 'Washington District', 'FLORIDA', 'MAINE', 'MASSACHUSETTS', 'MICHIGAN', 'NEW JERSEY', 'NEW YORK', 'Philadelphia', 'RHODE ISLAND', 'UTAH', 'VERMONT', 'VIRGINIA', 'Virgin Islands', 'WEST VIRGINIA', 'barbeque', 'Australia', 'Kent', 'Moscow', 'The United States', 'east Aleppo', 'Sallisaw', 'Oklahoma', 'Leavenworth', 'california', 'brooklyn', 'Sweden', 'Netherlands', 'North Dakota', 'Flint', 'theatre', 'Vermont', 'Austria', 'Schlosberg', 'United States', 'East Africa', 'Namibia', 'Botswana', 'South Africa', 'Zimbabwe', 'Mozambique', 'Insular Importance', 'Canary Islands', 'Spain', 'Madrid', 'Morocco', 'West Africa', 'Nigeria', 'South Sudan', 'Cameroon', 'Chad', 'the Democratic Republic of the Congo', 'Ethiopia', 'Uganda', 'South Kordofan', 'Burundi', 'Rwanda', 'Tanzania', 'Black Hole', 'Chaos', 'Ireland', 'Ketamine', 'Kildare', 'Crimea', 'All-Russia', 'the City of Santa Ana', 'Santa Ana', 'marijuana', 'Pharma', 'Guatemala', 'libido', 'BMJ', 'Lobbying Fees', 'Aging', 'Boston', 'Montenegro', 'Montenegrin', 'Serbia', 'Jeddah', 'Kirkuk', 'Riyadh', 'Argentina', 'Ecuador', 'https://t.co/octsMseme1', 'VA', 'Tecate', 'Kassel', 'Des Moines', 'Hollywood', 'the Ottoman Empire', 'Oligarchy', 'Poland', 'Romania', '@BraddJaffy', 'Omran', 'Qaterji', 'Trendelberg', 'Lincolnton', 'NC', 'Lincoln County', 'Charlotte', 'Ohio', 'Plum City', 'B.C.', 'Saudi Governments', 'Arizona', '@HillaryClinton', 'Epstein', 'Palm Beach', 'Mail', 'Pickton', 'Houston', 'New Westminster', 'Burnaby', 'Oakville', 'South City Slam', 'Port Coquitlam', 'Utah', 'Vietnam', 'Rosslyn', 'Va.', 'Baghdad', 'Kabul', 'Source', 'Estonia', 'the Järva County', 'Lethbridge', 'Canada', 'Scotland', 'Einstein', 'Fort Marcy Park', 'D.C.', 'the West hates', 'the AngloZionist Empire', 'Kiev', 'USA', 'Ukie', 'Ukrainian', 'Switzerland', 'the United Kingdom', 'The United States Govt', 'Fountain Hills', 'Austin', 'Eurasia', 'Latin', 'Albania', 'Ferguson', 'Staten Island', 'Cleveland', 'Hamilton', 'U.K.', 'Nebraska', 'Connecticut', 'Til Madîq', 'Hecinê', 'Qarami', 'Jabal', 'Douma', 'Alexandria', 'Brexit', 'the Weimar Republic', 'Italy', 'Greece', 'Saumur', 'Daesh', '\\x93Congratulations', 'Slovenia', 'Tiffany', 'the United States of America', 'Mosul', 'Miami', 'Umpire', 'San Diego', 'Nicaragua', 'the Dominican Republic', 'Hiroshima', 'Nagasaki', 'Chile', 'the Soviet Union', 'Victoria', 'Bari', 'Ilya', 'FL', 'Gaëtan', 'San Mateo', 'Copacabana', 'Medellin', 'Colombia', 'Bogota', 'Mumford', 'Cassandra', 'Norway', 'Prague', 'Oslo', 'San Francisco', 'Concord', 'Phoenix', 'Smartphones', 'Doomsday', 'Chicago', 'Syria?If', 'Hakone', 'Kanagawa', 'Whiskas', 'Maryland', 'Tarrant County', 'Quran', 'Barrick', 'Goldstrike', 'Munich', 'the West Bank', 'West Virginia', 'Valley Forge', 'Massachusetts', 'Martha\\x92s Vineyard', 'Wikileaks', 'the Russian Federation', 'Philippines', 'Assad', 'Antalya', 'propaganda', 'Chisinau', 'Moldova', 'the Republic of Moldova', 'Moldovan', 'Azerbaijan', 'Tbilisi', 'the Union State of Russia', 'Belarus', 'post-Soviet Moldova', 'Transnistria', 'Brussels', 'Transdnistria', 'Tiraspol', 'Bucharest', 'Donbass', 'Sprott', 'Toronto', 'goat', 'Cow', 'Great Britain', 'Kansas City', 'Alphabet', 'Jacksonville', 'Oklahoma City', 'Portland', 'San Jose', 'Tampa', 'Atlanta', 'Nashville', 'Provo', 'Salt Lake City', 'Huntsville', 'Alabama', 'Irvine', 'San Antonio', 'Louisville', 'Oakland', 'Bayonne', 'Merseyside', 'England', 'Gaddafi', 'The Netherlands', 'Stockholm', 'the State of Ecuador', 'Brennan', 'Mecca', 'San Bernardino', 'Tallahassee', 'Chapel', 'Observer', 'Paris', 'The Hague', 'Cambridge', 'Wikipedia', 'Sirte', 'Kremlin', 'Fort', 'the Russian Empire', 'Destefano', 'Vienna', 'Soccer', 'Marriage Cases', 'TEHRAN IRANCongress', 'PERM', 'New York State', 'the Federal United States District Court', 'the Federal United States District', 'the United States District', 'the United States-India', 'the Queens District', 'Showcases', 'Hindi, Urdu', 'the City of Portland', 'Slovakia', 'East Portland', 'Wapakoneta', 'turkey', 'Kentucky', 'Berlusconi', 'Venice\\x94', 'Assange', 'Illinois', 'Thailand', 'MANILA', 'Maps Eric Zuesse', 'U.S', 'Sauds', 'Anadolu', 'West Point', 'Teneo', 'Denmark', 'VE', 'Kosovo', 'Adams', 'Orlando', 'Fresno', 'boycotts', 'Benghazi', 'Cincinnati', 'Priest', 'Feyisa', 'Lilesa', 'Washington D.C.', 'Oromia', 'Ogaden', 'Amhara', 'Rio', 'the Peoples Republic of China', 'PRC', 'Yevgeny', 'United Russia', 'Black Female', 'the Hermit Kingdom', 'Pyongyang', 'Indiana', 'North Vietnam', 'South Vietnam', 'The New Republic', 'College Station', 'Beach', 'D-Indiana', 'Capito', 'Mansfield', 'Colorado Springs', 'Colorado', 'Cruz', 'Doubletree', 'Jefferson County', 'Tennessee', 'Gitmo', 'Quebec', 'Ottawa', 'Sydney', 'the Islamic State', 'Sub-Saharan Africa', 'AQAP', 'RNC', 'Pittsburgh', 'South Carolina', 'Wyoming', 'Washington County', 'Irion County', 'Oregon', 'Geneva County', 'Casey County', 'Whitley County', 'Snowden', 'Arlington', 'western Africa', 'Chattanooga', 'Bledsoe', 'Mexico City', 'Dayton', 'Fayetteville', 'N.C.', 'St. Louis', 'Montgomery County', 'Fairborn', 'Largo', 'Fla.', 'Sharonville', 'Ballwin', 'Mo.', 'Corden', 'Stalin', 'Polyarchy', 'Lakeland', 'Indianapolis', 'Noblesville', 'New Mexico', 'Warren', 'Fiorina', 'The Lone Star State', 'River City', 'Ky.', 'N.Y.', 'Atlantic City', 'N.J.', 'Jim Wells County', 'Dilley', 'Berks County', 'El Salvador', 'Huma', 'R-Iowa', 'Marijuana', 'New York Democratic Sens', 'Kobani', 'Garland', 'New Jersey', 'Deb Fischer', 'Richmond', 'Don Rickles', 'D-Calif.', 'Mich.', 'Malloy', 'Omaha', 'Detroit', 'Idaho', 'Hawaii', 'Minnesota', 'Pa.', 'Bethesda', 'Delaware', 'Rhode Island', 'Rockville', 'Md.', 'Kingsville', 'Williamsburg', 'the Golden State', 'Spreckels', 'the Garden State', 'Monterey', 'Eagle Pass', 'San Bernardino County', 'Sanctuary City', 'Norfolk', 'S.C.', 'Manchester', 'Columbia', 'non-U.S.', 'Laghman province', 'Milwaukee', 'Lausanne', 'Ramadi', 'Afrin', 'Makhmour', 'Nineveh', 'Kurdistan', 'Najaf', 'Baiji', 'the Sunni Islamic State', 'Anbar', 'Fallujah', 'Chait', 'Farmville', 'the Islamic Republic', 'North', 'Warsaw', 'Kondik', 'Lexington', 'Honolulu', 'Walmart.', 'the District of Columbia', 'Bernie', 'Dukakis', 'Obamanomics', 'Hamptons', 'west London', 'Charleston', 'Shelby', 'Metaxas', 'May.', 'Biden', 'Dina', 'New Orleans', 'Hughes', 'Winning Iowa', 'Santorum', 'Palinkas', 'Crystal City', 'Indyk', 'Barbados', 'Jamaica', 'Queens', 'Newtown', 'Conn.', 'Aurora', 'Oak Creek', 'Wis.', 'BEAUFORT', 'Guadalcanal', 'Bataan', 'Yorktown', 'Denver', 'Baltimore', 'Neb.', 'Fort Worth', 'Mesa', 'Ariz.', 'Tex.', 'Virginia Beach', 'Belgium', 'Bakraoui', 'Charleroi', 'Lawrence', 'Windsor', 'Brownsville', 'Kentucky county', 'Rowan County', 'Ashland', 'Brig', 'Tikrit', 'Anbar province', 'Charleston County', 'Konzny', 'Lexington County', 'Zawahiri', 'Darna', \"the Islamic State's\", 'Deir e-Zor', 'Amman', 'St. Louis County', 'Earth City', 'Belmar', 'Franklin County', 'Moneta', 'Overton', 'Fauquier County', 'Rocky Mount', 'Martinsville', 'communications@manhattan-institute.org', \"New Hampshire's\", 'Alaska', 'Athens', 'Berlin', 'us', 'Durham', 'Beverly Hills', 'Mission Viejo', 'Calif.', 'Greensboro', 'Palm Springs', 'Oscars', 'Pleasanton', 'Berkeley', \"Windy City's\", 'USC', 'Los Angeles City Hall', \"New York's\", \"the United Kingdom's\", 'Portugal', 'Tunisia', 'Eritrea', 'Cairo', 'Cyprus', 'MH370', 'Dimondale', 'Lansing', 'Panama', 'R-N.Y.', 'R-Ariz', 'R-Idaho', 'Labrador', 'Tuolumne', 'Tuolumne County', 'Bera', 'Abaaoud', 'Miami Dade', 'Ankara', 'Istanbul', 'Turkmenistan', 'the Cayman Islands', 'Arapahoe County', 'Rochester', 'Tamerlan', 'MA', 'Pornhub', 'Craigslist', 'Dzhokhar', 'Grozny', 'Chechnya', 'the Empire State', 'Upstate New York', 'Bataclan', 'graffiti', 'Charb', 'Ginsburg', 'the Rowan County', 'Kentucky Clerk', 'Hungary', 'Croatia', 'Tabak', 'Bosnia', 'Damascus', 'Calgary', 'Sioux Center', 'N.H.', 'Presidio County', 'Keene', 'D-Texas', 'Washington State', 'Litchfield', 'Christie']\n"
     ]
    }
   ],
   "source": [
    "# Your Answer Here \n",
    "list_of_ne = []\n",
    "\n",
    "for doc in processed:\n",
    "    named_entities =  [ent.text for ent in doc.ents if ent.label_ is 'GPE']\n",
    "    for ent in named_entities:\n",
    "        if ent not in list_of_ne:\n",
    "            list_of_ne.append(ent)\n",
    "            \n",
    "print(list_of_ne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# <font color=\"green\">Do You See Something That Could be Better?</font>\n",
    "\n",
    "I am committed to collecting student feedback to continuously improve this course for future students. I would like to invite you to help me make those improvements. \n",
    "\n",
    "As you worked on this module, did you notice anything that could be improved? For example, did you find a typo in the module notebook **or in the assigned reading**? Did you find the explanation of a particular concept or block of code confusing? Is there something that just isn’t clicking for you? \n",
    "\n",
    "If you have any feedback for the content in this module, please enter it into the text block below. I will review feedback each week and make a list of things that should be changed before the next offering. \n",
    "\n",
    "Please know that *nothing you say here, however critical, will impact how I evaluate your work in this course*. There is no risk that I will assign a lower grade to you if you provide critical feedback. In fact, if the feedback you provide is thoughtful and constructive, I will assign up to 3% bonus marks on your final course grade. \n",
    "\n",
    "Thanks for your help improving the course! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Feedback Here :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b3af578693290181",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "# REFERENCES <a id='refs'></a>\n",
    "\n",
    "* McLevey, John. 2020. *Doing Computational Social Science*. Sage. London, UK. \n",
    "* Muller, C., Sampson R.J., and Winter, A.S. (2018). 'Environmental Inequality: The Social Causes and Consequences of Lead Exposure.' *Annual Review of Sociology* (44) pp 263-282.\n",
    "* Nelson, Laura. 2017. 'Computational Grounded Theory: A Methodological Framework.' *Sociological Methods & Research*. 1:40. \n",
    "* Risdale, Megan. 2016. \"Getting Real about Fake News. Text & metadata from fake & biased news sources around the web.\" Dataset available on Kaggle: https://www.kaggle.com/mrisdal/fake-news "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

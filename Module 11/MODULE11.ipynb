{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-19fa5a00cb2bc31c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<br><br><font color=\"gray\">INTEG 440 / 640<br>MODULE 11 of *Doing Computational Social Science*</font>\n",
    "\n",
    "\n",
    "# <font color=\"green\" size=40>CLASSIFYING TEXT <br>WITH SUPERVISED LEARNING</font><br>\n",
    "\n",
    "Dr. [John McLevey](http://www.johnmclevey.com)    \n",
    "Department of Knowledge Integration   \n",
    "Department of Sociology & Legal Studies     \n",
    "University of Waterloo         \n",
    "\n",
    "<hr>\n",
    "\n",
    "* INTEG 440 (Undergraduate): This module is worth <font color='#437AB2'>**8%**</font> of your final grade. The questions in this module add up to 10 points. \n",
    "* INTEG 640 (Graduate): This module is worth <font color='#437AB2'>**5%**</font> of your final grade. The questions in this module add up to 10 points.  \n",
    "\n",
    "<hr>\n",
    "\n",
    "# Table of Contents \n",
    "\n",
    "* [Overview](#overview)\n",
    "* [Learning Outcomes](#lo) \n",
    "* [Prerequisite Knowledge](#pk) \n",
    "* [Assigned Reading](#ar) \n",
    "* [Question Links](#ql)\n",
    "* [Packages Used in this Module](#packs)\n",
    "* [Data Used in this Module](#data)\n",
    "* **[Key Concepts and Overall Process](#concepts)**\n",
    "* **[A Simple (and Unrealistic) Example](#unrealistic)**\n",
    "* **[A More Realistic Application](#realistic)**\n",
    "* [References](#refs)\n",
    "\n",
    "<hr>  \n",
    "\n",
    "\n",
    "# Overview <a id='overview'></a>\n",
    "\n",
    "The unsupervised methods of discovery introduced in the previous module offer the researcher very little control. For example, you can modify the parameters of a topic model to identify topics at various levels of abstraction (i.e. more or less specific), but you can't ask a topic model to estimate the prevalence of some specific topic -- say anti-immigration discourses -- in a corpus. \n",
    "\n",
    "By contrast, we can use supervised learning to scale up more traditional models of content analysis. This involves conceptualization, operationalization, and measurement (all covered at the start of the course) on the front end of a workflow. We then train a model to learn the complex relationships between the codes we have developed and the text. We can assess how well this model learned, and then -- if it learned well enough -- we can use it to classify text that we have not yet seen. \n",
    "\n",
    "As you now know, unsupervised learning is about discovery. Supervised learning for text analysis is largely about classification and confirmation. \n",
    "\n",
    "<hr>\n",
    "\n",
    "# Learning Outcomes  <a id='lo'></a>\n",
    "\n",
    "Upon successful completion of this module, you will be able to: \n",
    "\n",
    "1. Compare the goals and logic of supervised learning and unsupervised learning in text analysis \n",
    "2. Explain the role of conceptualization, operationalization, and measurement in supervised learning \n",
    "3. Conduct and interpret a supervised learning analysis \n",
    "\n",
    "<hr>\n",
    "\n",
    "# Prerequisite Knowledge  <a id='pk'></a>\n",
    "\n",
    "This module assumes comfort with the fundamentals of Python, and with the vectorization processes introduced in the module \"Exploratory Text Analysis.\"  \n",
    "\n",
    "<hr>\n",
    "\n",
    "# Assigned Readings  <a id='ar'></a>\n",
    "\n",
    "This module assumes you have completed the assigned readings, which are listed immediately below. The readings provide a detailed explanation of the core concepts covered in this module. \n",
    "\n",
    "* <font color=\"green\">Chapter 19 \"Supervised Learning and Scaling Up what Humans Do Best\" from *Doing Computational Social Science*.</font> \n",
    "\n",
    "As always, I recommend that you (1) complete the assigned readings, (2) attempt to complete this module without consulting the readings, making notes to indicate where you are uncertain, (3) go back to the readings to fill in the gaps in your knowledge, and finally (4) attempt to complete the parts of this module that you were unable to complete the first time around.\n",
    "\n",
    "This module notebook includes highly condensed overviews of *some* of the key material from the assigned reading. This is intended as a *supplement* to the assigned reading, *not as a replacement for it*. These high-level summaries do not contain enough information for you to successfully complete the exercises that are part of this module, and they do not cover every relevant topic.\n",
    "\n",
    "<hr>\n",
    "\n",
    "# Question Links <a id='ql'></a>\n",
    "\n",
    "Make sure you have answered all of the following questions before submitting this notebook on LEARN. \n",
    "\n",
    "1. [Question 1](#yt1) \n",
    "2. [Question 2](#yt2) \n",
    "3. [Question 3](#yt3) \n",
    "4. [Question 4](#yt4) \n",
    "5. [Question 5](#yt5) \n",
    "6. [Question 6](#yt6) \n",
    "7. [Question 7](#yt7) \n",
    "\n",
    "<hr>\n",
    "\n",
    "<a id='packs'></a>\n",
    "# Packages Used in this Module \n",
    "\n",
    "The cell below imports the packages that are necessary to complete this module. If there are any additional packages you wish to import, you may add them to this import cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import metaknowledge as mk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Used in this Module <a id='data'></a>\n",
    "\n",
    "In the second part of this module, we will use a dataset on 13 years of article publications in five peer reviewed journals: *Social Studies of Science*; *Science, Technology, & Human Values*; *Scientometrics*; *Journal of Informetrics*; and *Research Policy*. Our goal will be to classify articles based on whether they use a qualitative or a quantitative methodology. The publication metadata is stored in a `csv` file called `supervised_science.csv` in the `data` directory. We will read it into memory later in this module. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Concepts and Overall Process <a id='concepts'></a>\n",
    "\n",
    "To properly understand supervised learning, it is *essential* that you have a firm grasp of (1) foundational concepts and (2) how those concepts relate to one another. To help ensure that is the case, take a moment to review the following concepts. Then we will walk through a small toy example before turning to a more realistic application of supervised learning.  \n",
    "\n",
    "* **supervised learning**\n",
    "  * In the context of text analysis, supervised learning is when you have some unstructured text and a set of annotations / labels. You want to learn the relationship between the language used (and not used) in the text of any given document and the label it has been assigned (often by a human). If the model you fit has high accuracy, you want to use it to predict the labels for other unseen / out of sample data that does not have a label. This effectively extends the abilities of human coders to classify text to collections that are far too large to read.\n",
    "* **labels**  \n",
    "  * The thing you want to predict with your model.\n",
    "* **training data** and **test data**\n",
    "  * To know how good your model is at classifying text, you need to split your data into training data and a test data. You know the labels for both. You train the model on the training data (i.e. get it to learn the relationship between the features and the label), then ask it to predict the labels for the test data. Since you know the true labels for the test data (maybe you assigned them yourself!) you can compare the predictions of the model with the true labels. This lets you validate your model, for example by computing an accuracy rate.\n",
    "* **vocabulary**\n",
    "  * The full set of unique words used across all documents in our corpus.\n",
    "* **document-term matrix**, or **feature matrix**\n",
    "  * Machine learning algorithms need quantitative features in the form of a matrix. You can't just give them unstructured text. Using functions such as `CountVectorizer`, scikit-learn will learn the vocabulary (i.e. every unique word) used in your corpus. These words become \"features\" in a document-term matrix created using the `transform` function. In this matrix, the rows are documents and the columns (\"features\") are words. If there are 2.5 million unique words used across all of the documents in your corpus, then scikit-learn will learn them when it learns the vocabulary, and then when you transform it to a document-term matrix, it will have 2.5 million features (i.e. columns). Again, the number of features depends on the number of unique words in the whole vocabulary across all documents in your corpus. The values (cells) in this matrix are usually counts of the number of times a given feature (word) appears in a given document. However, the values may also be weights, such as TF-IDF.\n",
    "* **`CountVectorizer`**\n",
    "  * The scikit-learn function that processes your raw text data and learns the vocabulary in your corpus. It is a \"feature extraction\" method because it is extracting features (unique words) from your raw data and computing values (counts of occurrences) for documents in the corpus.\n",
    "* **`TfidfVectorizer`**\n",
    "  * Like `CountVectorizer` except it computes TF-IDF weights for each word. If you do not remember what TF-IDF is, please review your notes from previous modules or consult the readings.\n",
    "* **fit**\n",
    "  * When we fit a vectorizer, it learns the vocabulary used in our corpus. When we fit a classification model (e.g. Naïve Bayes) it learns the relationship between the features and the labels in order to make predictions about the labels given new data that is not yet labelled / annotated.\n",
    "* **transform** <!-- a sparse representation -->\n",
    "  * When you convert unstructured text into a document-term matrix of features.\n",
    "* **fit and transform** vs. just **transform**\n",
    "  * When you fit and transform text at the same time using `CountVectorizer` or `TfidfVectorizer`, scikit-learn learns the vocabulary used in the corpus and then transforms the text into a term-document matrix where the features are derived from the vocabulary. Once the vocabulary is learned and the model is trained, any new data will have to be transformed into a document-term matrix that includes *exactly* the same feature set that the original data was trained on. The model does not know about the relationship between any new features and the labels. So, when you are going to do prediction (see below), you need to convert the new data into a document-term matrix with the same feature set as the original data. You do **not** learn new features. So, you transform without fitting / learning.\n",
    "* **classification models**\n",
    "  * Once you have the document-term feature matrix, you can use classification models for supervised learning to learn the relationship between features and labels. There are *many* models to choose from, and scikit-learn makes it easy to apply any of them to the document-term matrices. Some choices include Multinomial Naïve Bayes, $k$-nearest neighbors, random forests, support vector machines, etc. The course readings describe these algorithms in detail. Make sure to consult them when you use these classifiers.\n",
    "* **out of sample data**\n",
    "  * The whole purpose of supervised learning is to be able to predict label values on new unlabelled data. That new data without labels is called \"out of sample data.\"\n",
    "* **predict**\n",
    "  * When you use the model trained on your training data to predict the value of labels that are unknown for the out of sample data.\n",
    "\n",
    "You may have noticed that it's hard to isolate some of these specific concepts from other concepts. That's because these concepts fit together in a coherent general process. If you understand the process, it becomes much easier to remember each specific concept.\n",
    "\n",
    "I have mapped out the general process for doing supervised machine learning with text in the figure below. I did it with scikit-learn in mind, and so it references specific scikit-learn functions like `CountVectorizer`. If you are using some other package (e.g. gensim), the general process is the same.\n",
    "\n",
    "![](img/supervised_process.png)\n",
    "\n",
    "As shown in the figure above, the process starts -- as always -- with acquiring and cleaning text data that has been classified using some set of labels (e.g. each text is labelled as being produced by a politician from one of several political parties). We then split our text into a training set and a testing set. We then \"learn\" the vocabulary in the dataset, create a document-term matrix with numerical features, fit a supervised learning model, and validate it against the testing data.\n",
    "\n",
    "You will learn how accurate your model is by seeing how it performs on the test data, which also contains the true labels. If your model is reasonably accurate, then you can use it to predict the labels using new out of sample data, which does *not* include information about the true labels.\n",
    "\n",
    "Let's see this whole thing in action. We will start simple and unrealistic, and then scale up to a more realistic analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple (and Unrealistic) Example <a id='unrealistic'></a>\n",
    "\n",
    "## Create Training and Test Data\n",
    "\n",
    "In this initial example, we are going to skip the step where we create training and test data. The only reason why we are doing that is because we want to use a really simple and small example that will help make the rest of the process as clear as possible. Once this example is done, we will go through a more realistic example, which will include splitting our data into training and testing sets, and then validating our classifier by computing accuracy rates and looking at a confusion matrix.\n",
    "\n",
    "## Vectorization: Transform Text into DTM\n",
    "\n",
    "We will start with the following data, which come from a workshop that Kevin Markham (2016) ran at PyData DC 2016: [Kevin Markham | Machine Learning with Text in scikit learn (1:24:19, Youtube)](https://www.youtube.com/watch?v=8QmkFAthuPU). **Our goal is to predict whether or not a text message is \"desperate.\"** We will label \"desperate\" as 1 and non-desperate as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = ['call you tonight', 'call me a cab', 'please call me... PLEASE!']\n",
    "is_desperate = [0, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As covered in the last class on processing and bag of words models, the first step in machine learning with text is *always* to go from raw text to a matrix of numerical features. Previously, we broke that down and did each task (e.g. tokenize, drop non-alpha, lower case, stem, etc.) separately and then created a bag of words. These bag of words can easily be turned into document-term matrices.\n",
    "\n",
    "To process and vectorize our text, we will use the `CountVectorizer` function we imported above. First, we will initialize it with the default settings (you can tune them later if you like), and then fit it to our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "vect.fit(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that `CountVectorizer` learns the vocabulary across all documents in our corpus. Let's see the words it learned from this toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cab', 'call', 'me', 'please', 'tonight', 'you']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in addition to tokenization and learning the vocabulary, `CountVectorizer` also pre-processed our text a bit. Punctuation is gone, words are lowercased, words less than 2 characters long are removed, every word is unique, and the words are rearranged to be in alphabetical order. We can change these settings if we want, and we can also add our own custom pre-processing to make this workflow *exactly* what we want.\n",
    "\n",
    "Now that we have learned the vocabulary and vectorized our text, we can *transform* it into a document-term matrix where the number of rows = the number of documents and the number of features (i.e. columns) = the number of words in the vocabulary. The values returned by `CountVectorizer` are, unsurprisingly, counts of the number of times the feature (i.e. word) appears in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_dtm = vect.transform(training_data)\n",
    "training_data_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our document-term matrix is 3 rows by 6 columns. Perfect. Let's take a look at the matrix via a Pandas dataframe. Note that I am not going to assign the Pandas dataframe to a variable, we are just going to take a peek at the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(training_data_dtm.toarray(), columns = vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this were a real example, there would be a lot more 0s because most words in your corpus will not be in most documents in your corpus. To save storage space and make computation more efficient, skikit-learn will store our data as a sparse matrix, meaning it only records information about non-0 values.\n",
    "\n",
    "In this example, I have separated the tasks of learning the vocabulary and transforming our data into a document-term matrix of numerical features. However, you can actually do all of these computations at the same time as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_dtm = vect.fit_transform(training_data)\n",
    "training_data_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 3 by 6 sparse matrix. We got the same results.\n",
    "\n",
    "## Train a Multinomial Naïve Bayes Classifier on the Document-Term Matrix\n",
    "\n",
    "Multinomial Naïve Bayes is a very popular classifier for text analysis. It's simple, effective, and has been around since the 1960s. The multinomial variant has been shown to be especially good for count vectors in text analysis.\n",
    "\n",
    "The Multinomial Naïve Bayes classifier is based on [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem). Maybe you remember it from your statistics classes?\n",
    "\n",
    "Basically, this Naïve Bayes model will classify the probability of the text being desperate or not desperate conditional on whether and how frequently these features (i.e. words) show up in our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(training_data_dtm, is_desperate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done. That's it!\n",
    "\n",
    "## Validate Model\n",
    "\n",
    "Normally at this point we would want to get a sense of how accurate our model is. We would do that by getting it to predict the values for our test data. We know the true values of the test data, so we can compare the estimates from our model with the truth.\n",
    "\n",
    "Again, we are not going to do real model evaluation on this toy example, so we will move onto the next step. However, we will do model evaluation in our more realistic example below.\n",
    "\n",
    "## Make Predictions on Out of Sample Data\n",
    "\n",
    "We now have a model that has learned, from our training dataset, the relationship between the features of our documents and the labels. If we were to show it some new out of sample data that is missing a label, we can predict the label. In order to do that, we have to transform the new data into a DTM (document-term matrix) with *identical* features to the one we used to train our model, and then make the prediction.\n",
    "\n",
    "Our new data will contain words that our trained model doesn't know. Those words will not be taken into consideration in our prediction because the model can't make predictions using information it doesn't know about. The DTM we use here must have *exactly* the same features as the DTM that we used to train our model. Therefore, we will transform our new data -- `new_text` -- into a DTM based on the vocabulary learned from our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = [\"please don't call me again\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `transform` method of the `CountVectorizer`, but not the `fit` method (which learns new words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text_dtm = vect.transform(new_text)\n",
    "new_text_dtm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the dataframe you will see that the features are in fact the same. New words ('again') are not included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   1       1        0    0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(new_text_dtm.toarray(), columns = vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the Multinomial Naïve Bayes model we trained to predict the class (desperate or not desperate) for this new data. We will do so using the `.predict` method. Earlier in the notebook, we called our classifier `nb_classifier,` so we put them together as `nb_classifier.predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier.predict(new_text_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier predicts that this is a desperate message, likely because of the word \"please.\" We humans know this is not the best choice. But in a more realistic application, we would have a lot more training data and if the model is tuned well it will make mistakes like this infrequently. Of course, we need to have good model validation to know how accurately it is classifying the out of sample data.\n",
    "\n",
    "## Using TF-IDF Weights Instead of Counts\n",
    "\n",
    "Before getting into a more realistic example, let's consider an alternative path we could have taken. The `CountVectorizer` transforms our data into a DTM where the values are counts of the number of times any given feature / word appears in any given document. Alternatively, we could have used `TfidfVectorizer`, which -- you guessed it -- transforms our data into a DTM where the values are TF-IDF weights instead of raw counts.\n",
    "\n",
    "Recall from previous modules that TF-IDF measures the importance of a given word in a given document in a larger corpus. Formally, it looks like this:\n",
    "\n",
    "$$w_i,_j = tf_i,_j * log\\Big(\\frac{N}{df_i}\\Big)$$\n",
    "\n",
    "Where the TF-IDF weight $w$ for word $i$ in document $j$ is equal to the term frequency multiplied by the inverse document frequency.\n",
    "\n",
    "The term frequency $tf_i,_j$ is just the number of times the word $i$ appears in document $j$. The inverse document frequency is the log of the total number of documents in the corpus divided by the number of documents in which the word $i$  appears. TF-IDF is just the product of those two numbers.\n",
    "\n",
    "So, broadly speaking, the weight of a word in a document increases the more frequently it appears in that document, but it decreases if it also appears across many other documents. Rare but not too rare words are weighted more than words that show up across many documents.\n",
    "\n",
    "Our toy example stays mostly the same as what we have already done, except that we use `TfidfVectorizer` instead of `CountVectorizer`. This time we will use the `fit_transform` method rather than separating the computations for `fit` and `transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x6 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "t_vect = TfidfVectorizer()\n",
    "tfidf_dtm = t_vect.fit_transform(training_data)\n",
    "tfidf_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Logistic Regression Classifier\n",
    "\n",
    "Multinomial Naïve Bayes works well when the values in a DTM are integers, but its performance is not quite as good when the values are floats. So, even though it will probably still work OK, we might want to try a different model. As mentioned previously, this is really easy to do in scikit-learn. Here we will use a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(tfidf_dtm, is_desperate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, nothing to test here... Let's just make a prediction of that new data introduced above: `new_text_dtm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.predict(new_text_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike our Multinomial Naïve Bayes model, our logistic regression model predicted that \"please don't call me again\" is not a desperate message.\n",
    "\n",
    "# Other Classifiers\n",
    "\n",
    "Before we move on to a more realistic example, take a few minutes to try out another classification method of your choice. You read about many in the readings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">YOUR TURN! (Question 1)</font> <a id='yt1'></a>\n",
    "\n",
    "Question is Worth: <font color=\"green\">2 points</font>\n",
    "    \n",
    "The assigned reading introduced a number of additional classifiers that we could also use here, such as support vector machines (SVMs) or random forests. \n",
    "\n",
    "In the cell below, select an additional classifier discussed in the readings and use it in the example we just walked through. Then compare the results to the Naïve Bayes and logistic regression classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your Answer Here \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(training_data_dtm, is_desperate)\n",
    "clf.predict(new_text_dtm)\n",
    "\n",
    "# Output is similar to the Logistic Regression Model in its opposite rather than the Naïve Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A MORE REALISTIC APPLICATION <a id='realistic'></a>\n",
    "\n",
    "\n",
    "\n",
    "Let's turn to a more realistic application.\n",
    "\n",
    "In this example, we have metadata on 12000 journal publications that are, broadly speaking, social scientific research on the structure, evolution, and content of science, and scientific careers and policy. The articles come from 5 journals: \n",
    "\n",
    "* *Scientometrics*\n",
    "* *Journal of Informetrics* \n",
    "* *Research Policy*\n",
    "* *Social Studies of Science*\n",
    "* *Science, Technology, & Human Values*\n",
    "\n",
    "The first two journals are exclusively quantitative, the latter two are mixed but are *almost* entirely qualitative (very few exceptions). The middle one? Well... let's find out... \n",
    "\n",
    "We will train a classifier to learn about differences in the language used in quantitative and qualitative research on science, and then predict the types of articles that get published in the journal *Research Policy*.\n",
    "\n",
    "Our **training data** is stored in a file called `supervised_science.csv` in the `data` directory. We can load it with `Pandas`. This dataset includes 300 articles each from all of our journals except *Research Policy*, which we will use for our out of sample data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>In this paper we deal with the problem of aggr...</td>\n",
       "      <td>quant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>Dyads of journals related by citations can agg...</td>\n",
       "      <td>quant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Ontology, and in particular, the so-called ont...</td>\n",
       "      <td>qual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>This paper builds on the growing literature in...</td>\n",
       "      <td>qual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>A new method of assessment of scientific paper...</td>\n",
       "      <td>quant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>A number of science and technology studies (ST...</td>\n",
       "      <td>qual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>Adaptation to the impacts of climate change is...</td>\n",
       "      <td>qual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>While in the beginning of the environmental de...</td>\n",
       "      <td>qual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Collins and Evans have proposed a normative th...</td>\n",
       "      <td>qual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>This paper develops a framework for understand...</td>\n",
       "      <td>qual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>Based on the curricula vitae and survey respon...</td>\n",
       "      <td>qual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>This article utilizes the ongoing debates over...</td>\n",
       "      <td>qual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Helen Verran uses the term relational empirici...</td>\n",
       "      <td>qual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>Based on historical citation data from the ISI...</td>\n",
       "      <td>quant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>Decision makers relying on web search engines ...</td>\n",
       "      <td>quant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>The objective of this study is to evaluate the...</td>\n",
       "      <td>quant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>New forms of life produced by biomedical resea...</td>\n",
       "      <td>qual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>Although there is increasing interest in polic...</td>\n",
       "      <td>quant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>In recent years, personality disorders - psych...</td>\n",
       "      <td>qual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>The US National Institute of Health's Human Mi...</td>\n",
       "      <td>qual</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "1030  In this paper we deal with the problem of aggr...  quant\n",
       "1100  Dyads of journals related by citations can agg...  quant\n",
       "156   Ontology, and in particular, the so-called ont...   qual\n",
       "174   This paper builds on the growing literature in...   qual\n",
       "1077  A new method of assessment of scientific paper...  quant\n",
       "75    A number of science and technology studies (ST...   qual\n",
       "514   Adaptation to the impacts of climate change is...   qual\n",
       "409   While in the beginning of the environmental de...   qual\n",
       "28    Collins and Evans have proposed a normative th...   qual\n",
       "127   This paper develops a framework for understand...   qual\n",
       "233   Based on the curricula vitae and survey respon...   qual\n",
       "511   This article utilizes the ongoing debates over...   qual\n",
       "219   Helen Verran uses the term relational empirici...   qual\n",
       "1155  Based on historical citation data from the ISI...  quant\n",
       "1122  Decision makers relying on web search engines ...  quant\n",
       "1063  The objective of this study is to evaluate the...  quant\n",
       "148   New forms of life produced by biomedical resea...   qual\n",
       "876   Although there is increasing interest in polic...  quant\n",
       "176   In recent years, personality disorders - psych...   qual\n",
       "312   The US National Institute of Health's Human Mi...   qual"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = pd.read_csv('data/supervised_science.csv')\n",
    "training_data.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our 1,200 observations include 600 quantitative articles and 600 qualitative articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quant    600\n",
       "qual     600\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Training Data and Test Data\n",
    "\n",
    "We have to split train-test before vectorization because we don't want to learn vocabulary in the test data. Basically, we are going to \"simulate the future\" as if this was new data coming in. That data will include words that are not in the vocabulary. We want to split the data and then vectorize so that we have a better simulation on when we have unknown words in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ignore the warning for now...\n",
    "X = training_data['text']\n",
    "y = training_data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "Let's learn the vocabulary for our analysis and then create a DTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 11135)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 900 documents (rows) and 11045 features (columns) in our matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 11135)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same shape, but 300 are held out for testing. So, to make sure it is clear, there are 1200 observations total in this data, 900 are for training and 300 for testing. If the tests come back with good results, then we will use this model to predict labels (quant/qual) for the out of sample data from *Research Policy*. \n",
    "\n",
    "## Train a Multinomial Naïve Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How accurate is our classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9833333333333333"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty accurate! 😎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>147</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  147    4\n",
       "1    1  148"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the assigned reading how to read a confusion matrix like the one presented above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9822222222222223"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "np.mean(cross_val_score(nb, X_train_dtm, y_train, cv=5, scoring =\"accuracy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">YOUR TURN! (Question 2)</font> <a id='yt2'></a>\n",
    "\n",
    "Question is Worth: <font color=\"green\">1 point</font>\n",
    "\n",
    "What does the confusion matrix above tell us about the accuracy of our classifications? Provide your answer in the cell below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix helps to tell us how accurately and correctly has the classifier classified each testing observation in accordance to the test labels that serve as a ground truth reference. Each row represents an actual class while each column represents a predicted class. Across the diagonals from left to bottom right, this represents true correctly classified values where the top left is true negative and bottom right is true positive. The other remaining cells are falsely classified values. If the truely classified diagonal is the only non-zero value in the matrix, then we have a perfect classifier. With the confusion matrix, we can calculate precision and recall to get a more informed insight on the classifier and the relative tradeoff on precision and recall which is variantly desired based on the nature of classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "567    The New York Times (NYT) receives more citatio...\n",
       "27     This paper examines the social capital that ev...\n",
       "386    This essay presents the first analysis of gend...\n",
       "401    In 1942, Katherine Frost Bruner published an a...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[y_test < y_pred_class]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">YOUR TURN! (Question 3)</font> <a id='yt3'></a>\n",
    "\n",
    "Question is Worth: <font color=\"green\">1 point</font>\n",
    "\n",
    "What does the code above (`X_test[y_test < y_pred_class]`) tell us about our classification? Provide your answer in the cell below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above helps to find those test observations where the test label is less than that of the predicted label. This includes all qualitative articles that were wrongly predicted as being those that are quantitative. These are all the false negative values, ie. those observation articles that are truefully qualitative pieces but were considered to contain quantitative components of composition. This helps us to identify errors and help with determining performance of the NB classifier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-88a733d05b17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0my_pred_class\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "[X_test, y_test, y_test < y_pred_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"qual\" < \"quant\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Naïve Bayes with Logistic Regression\n",
    "\n",
    "Let's now train and fit a logistic regression classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_dtm, y_train)\n",
    "\n",
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  148    3\n",
       "1    7  142"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9800000000000001"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.mean(cross_val_score(logreg, X_train_dtm, y_train, cv=5, scoring =\"accuracy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">YOUR TURN! (Question 4)</font> <a id='yt4'></a>\n",
    "\n",
    "Question is Worth: <font color=\"green\">1 point</font>\n",
    "\n",
    "In the cell below, translate the code from above (training and fitting the logistic regression classifier) into plain English. In terms of accuracy, how does it compare to the Naïve Bayes classifier? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the logistic regression classifier is initialized to a variable called `logreg`. With `logreg`, the classifier fits the data for the DTM matrix consisting of all the unique words across training articles along with a pertaining training label. This allows for predicting out of training data for probability scores or discrete classification based on the argument to be passed. With the classifier fine-tuned and fitted to the training data, the next step is to predict labels for the test DTM matrix with the `logreg` classifier and store into a variable called `y_pred_class`. To assess the accuraacy of classification, the sklearn package has a metrics class which measures the accuracy, as in the number of correctly indentified labels over the total labels within the predicted set. The logistic regression classifier performed at an accuracy of 97% in comparison to a 98% performance from the Naïve Bayes classifier. This means that the Naïve Bayes classifier has performed slightly better then LogReg. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the Types of STS Published in Research Policy\n",
    "\n",
    "Let's load our 'out of sample' data from *Research Policy* into a list called `RP`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1269 articles in our out of sample data from Research Policy.\n"
     ]
    }
   ],
   "source": [
    "pickle_in = open(\"data/research_policy.pickle\",\"rb\")\n",
    "RP = pickle.load(pickle_in)\n",
    "print(\"There are {} articles in our out of sample data from Research Policy.\".format(len(RP)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use both of our classifiers to predict the labels for our out of sample data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "RP_dtm = vect.transform(RP)\n",
    "lp = pd.DataFrame(logreg.predict(RP_dtm))\n",
    "nbp = pd.DataFrame(nb.predict(RP_dtm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quant    964\n",
       "qual     305\n",
       "Name: prediction, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp.columns = ['prediction']\n",
    "nbp.columns = ['prediction']\n",
    "\n",
    "lp['prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quant    1023\n",
       "qual      246\n",
       "Name: prediction, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbp['prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quant    964\n",
       "qual     305\n",
       "Name: prediction, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp['prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">YOUR TURN! (Question 5)</font> <a id='yt5'></a>\n",
    "\n",
    "Question is Worth: <font color=\"green\">1.5 points</font>\n",
    "\n",
    "What conclusions can we draw from these two classifiers about the content of *Research Policy*? How confident are you in those conclusions? Write your answer in the cell below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the `value_count` for each classifier, it seems that the general trend it that there have been predicted more quantitative then qualitative articles for the premise of classifying content of Research Policy. We have noticed from before that the Naïve Bayes classifier has performed at a better accuracy then the Logistic Regression classifier on the pertaining test data split from out training set. This is potentially indicative here where NB has been able to classify more \"quant\" than \"qual\" articles from Research Policy. From the confusion matrix between each classifier, it can be seen that each classifier has a similar precision but the NBP has a higher recall, thus it has detected more of the range of quantitative content and been able to precisely classify. Based on an average of a 5 cross fold validation, the NBP slightly beat the LogReg classifier. Based on these observations, it can be said with a relatively strong amount of confidence that the NB classifier performed better in the general observation; that there are more quantitative articles within Research Policy journal based on the words within the corpus across the documents. \n",
    "\n",
    "LogReg:\n",
    "Recall - 95.3%\n",
    "Precision - 97.9%\n",
    "\n",
    "NBP: \n",
    "Recall - 99.3%\n",
    "Precision - 97.4%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">YOUR TURN! (Question 6)</font> <a id='yt6'></a>\n",
    "\n",
    "Question is Worth: <font color=\"green\">2.5 points</font>\n",
    "    \n",
    "In the cell below, train and test a Support Vector Machine (SVM) classifier and use it to make predictions on the out of sample data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9788888888888889"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your Answer Here\n",
    "\n",
    "#Import svm model\n",
    "from sklearn import svm\n",
    "\n",
    "#Create a svm Classifier\n",
    "clf_svm = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf_svm.fit(X_train_dtm, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf_svm.predict(X_test_dtm)\n",
    "\n",
    "np.mean(cross_val_score(clf_svm, X_train_dtm, y_train, cv=5, scoring =\"accuracy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quant    946\n",
       "qual     323\n",
       "Name: prediction, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv = pd.DataFrame(clf_svm.predict(RP_dtm))\n",
    "sv.columns = ['prediction']\n",
    "sv['prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">YOUR TURN! (Question 7)</font> <a id='yt7'></a>\n",
    "\n",
    "Question is Worth: <font color=\"green\">1 point</font>\n",
    "\n",
    "Compare the accuracy of your SVM classifier with the Naïve Bayes and Logistic Regression classifiers. Which do you prefer and why? Provide your response in the cell below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM classifier performs at a slightly lesser accurate level; on the assumption of the conclusion drawn of there being more quant than qual as shown by NB and logReg classifiers. Based on cross_fold_validation averages, the SVM has performed slightly less than the others which is why I would take the Naïve Bayes classifier apart from the others. The NB classifier works on the assumption that all features are independent of one another and its relatively intepretable in how it functions with conditional probability being the driver for its decision making. Each word is distinct and independent, and based on the NB formula, the algorithm can encompass a large number of features and is relatively efficient in its processing as a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# <font color=\"green\">Do You See Something That Could be Better?</font>\n",
    "\n",
    "I am committed to collecting student feedback to continuously improve this course for future students. I would like to invite you to help me make those improvements. \n",
    "\n",
    "As you worked on this module, did you notice anything that could be improved? For example, did you find a typo in the module notebook **or in the assigned reading**? Did you find the explanation of a particular concept or block of code confusing? Is there something that just isn’t clicking for you? \n",
    "\n",
    "If you have any feedback for the content in this module, please enter it into the text block below. I will review feedback each week and make a list of things that should be changed before the next offering. \n",
    "\n",
    "Please know that *nothing you say here, however critical, will impact how I evaluate your work in this course*. There is no risk that I will assign a lower grade to you if you provide critical feedback. In fact, if the feedback you provide is thoughtful and constructive, I will assign up to 3% bonus marks on your final course grade. \n",
    "\n",
    "Thanks for your help improving the course! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Feedback Here :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# REFERENCES <a id='refs'></a>\n",
    "\n",
    "* Markham, Kevin. 2016. \"Kevin Markham | Machine Learning with Text in scikit learn (1:24:19, Youtube).\" PyData DC 2016. [Available online](https://www.youtube.com/watch?v=8QmkFAthuPU). \n",
    "* McLevey, John. 2020. *Doing Computational Social Science*. Sage. London, UK. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
